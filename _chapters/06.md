# Ch 6: Partitioning

> partition AKA: 
> - shard (MongoDB, Elastic, SolrCloud)
> - region (HBase)
> - tablet BIgTable
> - vnode (Cassandra, Riak)
> - vBucket (Couchbase)

Main Reasons for partitions: `scalability`
> Different partitions can be placed on different nodes in a shared-nothing cluster

## Partitioning VS replication
![image](https://github.com/friendlyantz/book-notes-designing-data-intensive-apps/assets/70934030/e5664217-4e4e-4008-a9c4-284653489ec8)

Often both combined. 
Although the choice of partitioning scheme is mostly independent
of the choice of replication scheme.
## Strategies (`key range` VS `hash of key`)

GOAL: to spread the data and the query load evenly across
nodes -> avoid skewed data, hot spots
> The simplest approach of avoiding hot spots would be to assign records to nodes randomly
> BUT
> when you’re trying to read a particular item, you have no way of knowing which node it is on, so you would have to query all nodes in parallel

### Partitioning by key range

![image](https://github.com/friendlyantz/book-notes-designing-data-intensive-apps/assets/70934030/c8095135-491d-42f7-989a-2675ecdbb8b7)


> Key range partitioning, where keys are sorted, and a partition owns all the keys
from some minimum up to some maximum. This has the advantage that effi‐cient range queries are possible, but there is a risk of hot spots if the application
often accesses keys that are close together in the sorted order.
In this approach, partitions are typically rebalanced dynamically, by splitting the
range into two sub-ranges when a partition gets too big.

### Partitioning by hash of key

![image](https://github.com/friendlyantz/book-notes-designing-data-intensive-apps/assets/70934030/2ae5382b-f6ba-436d-bb44-7c71a24dad98)


>Hash partitioning, where a hash function is applied to each key, and a partition
owns a range of hashes. This destroys the ordering of keys, making range queries
inefficient, but may distribute load more evenly.
When partitioning by hash, it is common to create a fixed number of partitions in advance, to assign several partitions to each node, and to move entire partitions from one node to another when nodes are added or removed.

---

> i.e 32-bit hash function which takes a string. Whenever you give it a new string, it returns a seemingly random number between 0 and 2 <sup>32</sup> − 1.

Hash function needs to be cryptographically strong:
- MD5 -> MongoDB, Cassandra
- Fowler-Noll-Vo func -> Voldemort

But Ruby's `Object#hash`  or Java's `Object.hashCode()` and other language build-in algos might not be suitable
> Note: might depend on the input of the Hash, if string - might be ok, not ok  

### Compound Primary Key (ref p204-205)

### Skewed workloads and relieving hot spots

> simple technique is to add a random number to the beginning or end of the key. 
> Just a two-digit decimal random number would split the writes to the key evenly across 100 different keys, allowing those keys to be distributed to different partitions. 
> However, having split the writes across different keys, any reads now have to do addi‐ tional work, as they have to read the data from all 100 keys and combine it. 
> This tech‐ nique also requires additional bookkeeping: 
> it only makes sense to append the random number for the small number of hot keys; for the vast majority of keys with low write throughput this would be unnecessary overhead. 
> Thus, you also need some way of keeping track which keys are being split.

---

Hybrid approaches (Key range partitioning + Hash partitions) are also possible, for example with a compound key: using one part of the key to identify the partition, and another part for the sort order
## Secondary indexes

> A secondary index usually doesn’t identify a record uniquely, but rather, it’s a way of searching for occurrences of a particular value

### Document-partitioned index (aka `local index`):

![image](https://github.com/friendlyantz/book-notes-designing-data-intensive-apps/assets/70934030/9d500810-4179-4ffb-9dcd-6059f28732f4)


> the secondary indexes are stored in the same partition as the primary key and value. This means that only a single partition needs to be updated on write, but a read of the secondary index requires a scatter/ gather across all partitions.

`scatter/gather`

used by: MongoDB, Riak , Cassandra ,Elasticsearch , SolrCloud , and VoltDB

Drawbacks:

> Even if you query the partitions in parallel, scatter/gather is prone to `tail latency amplification `.

> Most database vendors recommend that you structure your partitioning scheme so that secondary index queries can be served from a single partition, but that is not always possible, especially when you’re using multiple secondary indexes in a single query (such as filtering cars by color and by make at the same time)

### Term-partitioned index (aka `global index`): 

![image](https://github.com/friendlyantz/book-notes-designing-data-intensive-apps/assets/70934030/f72717bf-648c-4550-88b8-c714dc485dcf)

> the secondary indexes are partitioned separately, using the indexed values. An entry in the secondary index may include records from all partitions of the primary key. When a document is written, sev‐ eral partitions of the secondary index need to be updated; however, a read can be served from a single partition.

---
advantage of a global (term-partitioned) index over a document-partitioned index:
- can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. 

downside of a global index:
- writes are now slower and more complicated, because a write to a single document may now affect multiple partitions of the index (every term in the document might be on a different partition, on a different node).
## Rebalancing partitions

> The process of moving data around between nodes in the cluster

Requirement:
• After rebalancing, the load (data storage, read and write requests) should be shared fairly between the nodes in the cluster.
• While rebalancing is happening, the database should continue accepting reads and writes.
• Don’t move more data than necessary between nodes, to avoid overloading the network.

### Strategies for rebalancing

How not to do it: hash `mod(N)

#### Fixed number of partitions

> create many more partitions than there are nodes, and assign several partitions to each node. For example, a database run‐ ning on a cluster of 10 nodes may be split into 1,000 partitions from the outset, so that approximately 100 partitions are assigned to each node. 
#### Dynamic partitioning

> the number of partitions is usually fixed when the database is
first set up, and not changed afterwards. Although in principle it’s possible to split
and merge partitions (see next section), a fixed number of partitions is operationally
simpler, and so many fixed-partition databases choose not to implement partition
splitting. Thus, the number of partitions configured at the outset is the maximum
number of nodes you can have, so you need to choose it high enough to accommo‐
date future growth. However, each partition also has management overhead, so it’s
counterproductive to choose too high a number.

#### Dynamic partitioning

#### Partitioning proportionally to the nodes

### Operations: automatic or manual rebalancing
>Fully automated rebalancing can be convenient, because there is less operational
work to do for normal maintenance. However, it can be unpredictable.
Rebalancing is an expensive operation, because it requires re-routing requests and moving a large
amount of data from one node to another. If it is not done carefully, this can overload
the network or the nodes, and cause performance problems for other systems.

<<<<<<< HEAD
> This can be dangerous in combination with automatic failure detection. For example,
say one node is overloaded and is temporarily slow to respond to requests. The other
nodes conclude that the overloaded node is dead, and automatically rebalance the
cluster to move load away from it. This puts additional load on the other nodes and
the network, thus potentially overloading more nodes and causing a cascading fail‐
ure.

=======
> This can be dangerous in combination with automatic failure detection. For example, say one node is overloaded and is temporarily slow to respond to requests. The other nodes conclude that the overloaded node is dead, and automatically re-balance the cluster to move load away from it. This puts additional load on the other nodes and the network, thus potentially overloading more nodes and causing a cascading failure.
>>>>>>> 7c91059 (UPD CH6 notes)
## Request routing

![image](https://github.com/friendlyantz/book-notes-designing-data-intensive-apps/assets/70934030/74e1b297-622f-4972-a9e2-b555940dd52a)


here are a few different approaches to this problem

1. Allow clients to contact any node (e.g. via a round-robin load balancer). If that
node coincidentally owns the partition to which the request applies, it can handle
the request directly; otherwise it forwards the request to the appropriate node.

2. Send all requests from clients to a routing tier first, which determines the node
that should handle the request and forwards it accordingly. This routing tier does
not itself handle any requests, it only acts as a partition-aware load balancer

3. Require that clients be aware of the partitioning and the assignment of partitions
to nodes. In this case, a client can connect directly to the appropriate node,
without any intermediary.
