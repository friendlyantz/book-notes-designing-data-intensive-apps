# Part II: Distributed Data

# Ch 5: Replication

## Intro

WHY:
1. Geo proximity to users
2. fail-safe -> increases __availability__ 
3. scale out -> increase __throughput__
Types:
- single-leader
- multi-leader
- leaderless
> Synchronous VS Async

## Leaders and Followers

> aka active/passive OR master-slave replication
Used in: 
- DBs: PostreSQL, MySQL as well  as MongoDB, etc etc
- Non DBs: Kafka, RabbitMQ

### Synchronous vs. asynchronous replication

- `Synchronous` generally means __one__ of the followers is only synced, since it is impractical to make all followers to sync as it will halt the system. It is then often called `semi-synchronous`
- Often thought, replication is `async` -> WRITES are not guaranteed to be durable, but writes are not slowed down by replication
- `chain replication` - sync replication with good performance ( used in MS Azure)

### Setting up new followers

Steps:
1. take snapshot (built in feature often, for MySQL use _innobackupex_)
2. copy snapshot to the follewer
3. update follower with a newer data after snapshot's exact pos in replication log ( aka `log sequence number` in PostgreSQL, `binlog coordinates` in MySQL )
4. confirm it `caught up`

### Handling node outages

1. Follower failure: catch-up recovery
	- follower recovers from it's `log`, which points to the last transaction processed and can copy newer data from the leader
2. Leader failure: `Failover`
	1. Determine Leader has Failed
		1. nodes bounce messages between each other
	2. Choose new leader
		1. make all nodes agree on a new leader(`consensus problem`)
		2. ideally new leader needs to have most up-to-date data
	3. Reconfigure the system to use new leader
		1. Ensure old leader becomes a follower after recovery
		ISSUES:
			- async replication looses leaders newest writes
			- may cause issues on depndent systems - i.e. GitHub MySQL-Redis issue
			- `split brain` issue - both leaders accept writes
			- Timeout time - Time for recovery VS False positives (during peak load)

> Perform Failovers manually to minimise issues

### Implementation of replication logs

#### 1. Statement-based replication

 Leader's log sent to all followers (it includes ever WRITE request (statement) - INSERT, UPDATE, DELETE for relational DB) so they can execute these statements themselves
	- Issues:
			1. `nondetermenistic` functions generate diff values - i.e. RAND(), NOW()
			2. `autoincrement` - should be in the same order on each replica
			3. `side effects` of statements (i.e. triggers, stored procedures, user-defined functions) 
#### 2. Write-ahead log (WAL) shipping

Append only log. In case of B-trees data is writtten into WAL then onto B-tree index
	- used in PostgreSQL, Oracle
	- Disadvantage: 
		- very low level representation of data -> replication is closely coupled with storage engine
		- hence can't have leaders and followers on different DB versions
#### 3. Logical (row-based) log replication
Contains
			- for inserts - new values
			- for deletions - data to id the row (PrimaryKey or values)
			- for updates - data to id the row + new values
Used in MySQL `binlog` when configured to use _row-based_ replication
Advantage:
			- decoupled from storage engine and can be backwards compatible or even use different storage engines
			- easier to use with external apps. Refer `change data captuer` Ch11
#### 4. Trigger-based replication

Unlike previous algos - it is defined in App and gives felixbility (i.e. Conflict Resolution, replicate only certain subsets of data)
Use built-in RDB features: `Triggers` and `Stored Procedures`: 
	exec custom code on WRITE transaction. Can log change into a separate table (i.e. `Bucardo` for Postgres) 

## Problems With Replication Lag

_Eventual Consistency_ - _Replication Lag_ can be seconds, but can be minutes+ under stress, resulting in the following 3 issues
### 1.Reading your own writes

Users might not see their changes / writes instantly, we need `read-after-write`(or `read-your-writes`) consistency.
Implementation techniques for  `read-after-write` consistency:
1. Read from leader if user is an author (easy for profile settings / etc since only one user generally can read them)
2. Track times of the last update and then decide if you need to read from leader
3. Client can remember the timestamp of the most recent WRITE and client query read replica accordingly. 
	1. this timestamp can be `logical timestamp` or actual system clock(Refer `Unreliable Clocks`)
Cross-device complexities:
- timestamp of user's last upd needs to be centralized
- Cell / radio network might go to a different datacentre than wire 
### 2. Monotonic reads
- lesser guarantee than `strong consistency`
- stronger guarantee than `eventual consistency`
Archie by making a user read from a same replicate assigned to them
### 3. Consistent prefix reads
- readers need to see writes in the same order
###  Solutions for replication lag

Transactions (txns)
- Single-node txns existed for a long time
	- however they can be expensive(performance & availability) in distributed systems\
	- and `eventual consistency` is inevitable some claim
## Multi-leader replication

> aka `master-master` or `active/active`
Leaders also act as followers to other leaders
### Use cases for multi-leader replication
#### Multiple Datacentres
Advantages over single-leader
1. __Performance__ -> lower latency due to locality of data-centres
2. __Datacentre Outage Tolerance__
3. Network Tolerance - followers 

Multi-leader configs - supported by default sometimes, but also can be done via ext tools:
- BDR for Postgres
- Tungsten Replicator for MySQL

Disadvantages of Multi-leader replication:
- requires `conflict resoluton` - as same data can be concurrently modified
- since it is often a retrofitted feature - it requires extra config (i.e auto-incrementing keys, triggers, integrity constraints)
#### Clients with Offline ops
> In this case, every device has a local database that acts as a leader (it accepts write
> requests), and there is an asynchronous multi-leader replication process (sync)
> between the replicas of your calendar on all of your devices. The replication lag may
> be hours or even days, depending on when you have internet access available.

[CouchDB](https://en.wikipedia.org/wiki/Apache_CouchDB) is designed for this mode of operation

#### Collaborative editing

> for faster collaboration, you may want to make the unit of change very
> small (e.g. a single keystroke), and avoid locking
### Handling write conflicts

#### Synchronous vs. asynchronous conflict detection
> Synchronous loses the main advantage of multi-leader repli‐
cation: allowing each replica to accept writes independently
#### Conflict avoidance
- route all changes for a particular record through a particular datacentre
- but if datacentre needs to be changed it poses an issue
#### Converging towards a consistent state
> all replicas must arrive at the same final value when all changes have been replicated.
Ways to achieve:
1. Give each write a unique ID (data loss)
	1. i.e. LWW - last write wins - refer _Detecting Concurrent Writes_
2. Give each replica a unique ID (data loss)
3. merge the values together
4. Record the conflict and solve later (i.e. by user)
#### Custom conflict resolution logic
- on write
- on read
#### Automatic Conflict Resolution

- Conflict-free replicated data types (CRDTs)
	- two-way merge function
- Mergeable persistent data structures
	- similar git version control system, and use a three-way merge function
- Operational transformation
	- Google Docs
#### What is a conflict?
### Multi-leader replication topologies

![image](https://github.com/friendlyantz/book-notes-designing-data-intensive-apps/assets/70934030/b2edb5cf-8d76-40bd-ba8e-505408e6d091)


## Leaderless replication

### Writing to the database when a node is down

### Limitations of quorum consistency

### Sloppy quorums and hinted handoff

### Detecting concurrent writes
